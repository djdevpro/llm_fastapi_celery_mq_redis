# ğŸš€ LLM Stream + RabbitMQ

> **Scalez votre API LLM de 50 Ã  1000+ requÃªtes simultanÃ©es** grÃ¢ce au dÃ©couplage via RabbitMQ.

[![Python 3.12](https://img.shields.io/badge/python-3.12-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.124-green.svg)](https://fastapi.tiangolo.com)
[![Docker](https://img.shields.io/badge/Docker-ready-blue.svg)](https://docker.com)
[![Tests](https://img.shields.io/badge/tests-8%2F8%20passed-success.svg)](#-tests)

## ğŸ¯ Le problÃ¨me rÃ©solu

Votre API LLM lag quand plusieurs utilisateurs envoient des requÃªtes simultanÃ©ment ? C'est normal : chaque appel OpenAI prend **10-60 secondes** et bloque un worker HTTP.

**Cette architecture rÃ©sout le problÃ¨me** en dÃ©couplant le traitement :
- L'API retourne **immÃ©diatement** (~100ms)
- Les workers LLM traitent les requÃªtes **en parallÃ¨le**
- Le client reÃ§oit la rÃ©ponse via **Server-Sent Events**

## ğŸ“Š Benchmarks rÃ©els

Tests exÃ©cutÃ©s avec 5 workers LLM :

| MÃ©trique | RÃ©sultat |
|----------|----------|
| Temps de rÃ©ponse API | **103ms** (fire & forget) |
| 5 requÃªtes parallÃ¨les | **50s** au lieu de 250s sÃ©quentiel |
| Gain de parallÃ©lisme | **5.0x** |
| Mode async vs sync | **13x** plus rapide pour libÃ©rer le serveur |
| Burst 10 requÃªtes | AbsorbÃ© en **613ms** |

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RÃ‰SULTATS DES TESTS                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Temps total (5 req):   49.99s                   â”‚
â”‚  Si sÃ©quentiel:         249.07s                  â”‚
â”‚  Gain parallÃ©lisme:     5.0x âœ…                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ¨ FonctionnalitÃ©s

- ğŸš€ **Streaming LLM** via OpenAI API (gpt-4o-mini)
- ğŸ“¡ **RabbitMQ** pour le dÃ©couplage producteur/consommateur
- ğŸ”„ **SSE** (Server-Sent Events) pour le streaming temps rÃ©el
- ğŸ³ **Docker** avec auto-scaling des workers
- âš™ï¸ **Configuration via ENV** : ajustez workers selon la charge
- ğŸ§ª **Tests pytest** : 8/8 tests validant le parallÃ©lisme

---

## ğŸ“ Architecture

### Mode Synchrone (`/chat`) â€” CompatibilitÃ©

```
Client â”€â”€POST /chatâ”€â”€â–º FastAPI â”€â”€appel OpenAIâ”€â”€â–º RÃ©ponse (bloque 10-60s)
```

### Mode Asynchrone (`/chat/async`) â€” Production âš¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    POST /chat/async   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚    FastAPI      â”‚
â”‚             â”‚ â—„â”€â”€ session_id â”€â”€â”€â”€â”€â”€ â”‚   (~100ms) âœ…   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                       â”‚
       â”‚                                       â–¼
       â”‚ SSE                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                              â”‚    RabbitMQ     â”‚
       â”‚                              â”‚   (llm_tasks)   â”‚
       â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                       â”‚
       â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                              â–¼        â–¼        â–¼
       â”‚                           Worker   Worker   Worker
       â”‚                           LLM #1   LLM #2   LLM #N
       â”‚                              â”‚        â”‚        â”‚
       â”‚ GET /stream/{id}             â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                       â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         chunks SSE
```

### Comparaison des modes

| Aspect | `/chat` (sync) | `/chat/async` |
|--------|----------------|---------------|
| Latence HTTP | 10-60s (bloquÃ©) | **~100ms** |
| Workers HTTP | 1 occupÃ© par requÃªte | LibÃ©rÃ© instantanÃ©ment |
| ScalabilitÃ© | LimitÃ©e | **Horizontale** |
| Use case | Dev/tests | **Production** |

---

## âš™ï¸ Variables d'environnement

| Variable | Description | DÃ©faut | Requis |
|----------|-------------|--------|--------|
| `OPENAI_API_KEY` | ClÃ© API OpenAI | - | âœ… |
| `RABBIT_MQ` | URL RabbitMQ (CloudAMQP ou local) | - | âœ… |
| `UVICORN_WORKERS` | Workers HTTP (routing) | `4` | âŒ |
| `LLM_WORKERS` | Workers LLM (traitement OpenAI) | `3` | âŒ |
| `PORT` | Port de l'API | `8007` | âŒ |

### Exemple `.env`

```env
# === REQUIS ===
OPENAI_API_KEY=sk-proj-xxxxx
RABBIT_MQ=amqps://user:pass@coral.rmq.cloudamqp.com/vhost

# === SCALING (optionnel) ===
UVICORN_WORKERS=4   # Workers HTTP
LLM_WORKERS=5       # Workers LLM (1 worker = 1 requÃªte OpenAI en parallÃ¨le)
PORT=8007
```

---

## ğŸš€ DÃ©marrage rapide

### 1. Configuration

```bash
cp .env.example .env
# Ã‰diter avec vos clÃ©s OpenAI et RabbitMQ
```

### 2. Build & Run

```bash
# Option 1 : Script
./run.sh start

# Option 2 : Docker manuel
docker build -t llm-fastapi-mq .
docker run -d --name llm-mq-poc \
  -p 8007:8007 \
  -e LLM_WORKERS=5 \
  --env-file .env \
  llm-fastapi-mq
```

### 3. VÃ©rification

```bash
# Health check
curl http://localhost:8007/health/full
# âœ… {"status":"ok","rabbitmq":"connected","openai":"configured"}

# Logs de dÃ©marrage
docker logs llm-mq-poc
# ========================================
#   LLM FastAPI + RabbitMQ
# ========================================
#   Uvicorn workers: 4
#   LLM workers:     5
# ========================================
```

---

## ğŸ“¡ API Endpoints

| MÃ©thode | Endpoint | Description |
|---------|----------|-------------|
| `GET` | `/health` | Health check basique |
| `GET` | `/health/full` | Statut RabbitMQ + OpenAI |
| `GET` | `/stats` | TÃ¢ches en attente |
| `POST` | `/chat` | Mode sync (legacy) |
| `POST` | `/chat/async` | **Mode async (recommandÃ©)** âš¡ |
| `GET` | `/stream/{session_id}` | Stream SSE des chunks |

### Exemple : Mode Async (recommandÃ©)

```bash
# 1. Envoie la requÃªte â†’ retour immÃ©diat (~100ms)
curl -X POST http://localhost:8007/chat/async \
  -H "Content-Type: application/json" \
  -d '{"message": "Explique Docker en 3 points"}'

# RÃ©ponse instantanÃ©e :
# {"status":"queued","session_id":"abc-123","stream_url":"/stream/abc-123"}

# 2. Ã‰coute le stream SSE
curl -N http://localhost:8007/stream/abc-123
# data: {"chunk": "Docker"}
# data: {"chunk": " est"}
# data: {"chunk": " un"}
# ...
# data: {"type": "done"}
```

---

## ğŸ§ª Tests

8 tests pytest validant le parallÃ©lisme :

```bash
# Dans le conteneur Docker
docker exec llm-mq-poc pytest tests/ -v -s

# Ou localement
pip install pytest pytest-asyncio httpx
pytest tests/ -v -s
```

### RÃ©sultats des tests

```
tests/test_concurrent.py::TestHealthCheck::test_health âœ…
tests/test_concurrent.py::TestHealthCheck::test_health_full âœ…
tests/test_concurrent.py::TestAsyncMode::test_chat_async_returns_immediately âœ… (103ms)
tests/test_concurrent.py::TestAsyncMode::test_stream_receives_chunks âœ…
tests/test_concurrent.py::TestParallelProcessing::test_parallel_5_requests âœ… (5.0x gain)
tests/test_concurrent.py::TestParallelProcessing::test_compare_sync_vs_async âœ… (13x)
tests/test_concurrent.py::TestLoadCapacity::test_queue_stats âœ…
tests/test_concurrent.py::TestLoadCapacity::test_burst_10_requests âœ… (613ms)

========================= 8 passed in 92.44s =========================
```

---

## ğŸ“Š Scaling

### Formule du nombre de workers LLM

```
LLM_WORKERS = (requÃªtes/minute) Ã— (temps moyen gÃ©nÃ©ration en minutes)

Exemple :
- 60 requÃªtes/minute attendues
- 30 secondes par gÃ©nÃ©ration (0.5 min)
- Workers = 60 Ã— 0.5 = 30 workers
```

### Configurations recommandÃ©es

| Charge | LLM_WORKERS | UVICORN_WORKERS | RAM |
|--------|-------------|-----------------|-----|
| Dev | 2 | 1 | 512 MB |
| Petit (10 users) | 5 | 2 | 1 GB |
| Moyen (50 users) | 15 | 4 | 2 GB |
| Production (100+ users) | 30-50 | 4 | 4-8 GB |

### Lancer avec plus de workers

```bash
docker run -d --name llm-prod \
  -p 8007:8007 \
  -e LLM_WORKERS=30 \
  -e UVICORN_WORKERS=4 \
  --env-file .env \
  llm-fastapi-mq
```

---

## ğŸ–¥ï¸ Interface Web

```bash
# Windows
start chat.html

# macOS / Linux
open chat.html
# ou
python -m http.server 3000 && open http://localhost:3000/chat.html
```

**FonctionnalitÃ©s :**
- ğŸ’¬ Chat temps rÃ©el avec streaming
- ğŸ”„ Switch mode RabbitMQ / Direct
- ğŸ“Š Indicateurs de statut (API, Queue, Stream)
- ğŸ“± Design responsive

---

## ğŸ“ Structure du projet

```
llm_fastapi_mq/
â”œâ”€â”€ main.py                 # API FastAPI
â”œâ”€â”€ config.py               # Variables d'environnement
â”œâ”€â”€ Dockerfile              # Image multi-workers
â”œâ”€â”€ entrypoint.sh           # Lance API + Workers auto
â”œâ”€â”€ chat.html               # Interface web
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ connection_pool.py  # Pool connexions RabbitMQ
â”‚   â”œâ”€â”€ llm_worker.py       # Worker LLM (scalable)
â”‚   â”œâ”€â”€ rabbit_publisher.py # Publisher
â”‚   â””â”€â”€ rabbit_consumer.py  # Consumer SSE
â””â”€â”€ tests/
    â”œâ”€â”€ conftest.py         # Config pytest
    â””â”€â”€ test_concurrent.py  # Tests parallÃ©lisme
```

---

## ğŸ› Troubleshooting

### RequÃªtes traitÃ©es sÃ©quentiellement

```bash
# VÃ©rifier les workers actifs
docker top llm-mq-poc | grep llm_worker

# Augmenter si nÃ©cessaire
docker run -e LLM_WORKERS=10 ...
```

### Erreur connexion RabbitMQ

```bash
# Plan CloudAMQP gratuit = 20 connexions max
# RÃ©duire les workers
docker run -e LLM_WORKERS=3 -e UVICORN_WORKERS=2 ...
```

### Erreur OpenAI (caractÃ¨res Windows)

```bash
# Nettoyer les \r du .env
sed -i 's/\r$//' .env
```

### Monitoring

```bash
# TÃ¢ches en attente
curl http://localhost:8007/stats
# {"pending_tasks":5,"queue":"llm_tasks","status":"ok"}

# Health complet
curl http://localhost:8007/health/full
```

---

## ğŸ“„ License

MIT

---

## ğŸ¤ Contribution

1. Fork le projet
2. CrÃ©er une branche (`git checkout -b feature/amazing`)
3. Commit (`git commit -m 'Add amazing feature'`)
4. Push (`git push origin feature/amazing`)
5. Ouvrir une Pull Request

---

<p align="center">
  <b>â­ Si ce projet vous aide, laissez une Ã©toile !</b>
</p>
