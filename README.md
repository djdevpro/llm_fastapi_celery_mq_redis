# LLM Stream + RabbitMQ

POC de streaming LLM avec dÃ©couplage via RabbitMQ. Permet de dÃ©charger le streaming des rÃ©ponses LLM du serveur principal vers des workers indÃ©pendants.

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     POST /chat      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚    FastAPI      â”‚
â”‚  (Browser)  â”‚                     â”‚    Service      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â–²                                     â”‚
       â”‚                                     â”‚ Publish chunks
       â”‚ SSE                                 â–¼
       â”‚                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                            â”‚    RabbitMQ     â”‚
       â”‚                            â”‚  (CloudAMQP)    â”‚
       â”‚                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                     â”‚
       â”‚ GET /stream/{session_id}            â”‚ Consume
       â”‚                                     â–¼
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                     â”‚   SSE Stream    â”‚
                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## FonctionnalitÃ©s

- ğŸš€ **Streaming LLM** via OpenAI API (gpt-4o-mini)
- ğŸ“¡ **RabbitMQ** pour le dÃ©couplage producteur/consommateur
- ğŸ”„ **SSE** (Server-Sent Events) pour le streaming client
- ğŸ³ **Docker** ready
- âš¡ **Deux modes** : RabbitMQ (async) ou Direct (sync)

## PrÃ©requis

- Docker
- Compte OpenAI (API Key)
- Compte CloudAMQP ou RabbitMQ local

## Installation

### 1. Configuration

```bash
cp .env.example .env
```

Ã‰diter `.env` :

```env
OPENAI_API_KEY=sk-your-openai-api-key
RABBIT_MQ=amqps://user:password@host/vhost
```

### 2. Build & Run

```bash
# Build et lancer
./run.sh start

# Ou manuellement
docker build -t llm-fastapi-mq .
docker run -d --name llm-mq-poc -p 8007:8007 --env-file .env llm-fastapi-mq
```

### 3. Test

```bash
# Health check
curl http://localhost:8007/health

# Test OpenAI
curl http://localhost:8007/test

# Chat avec streaming
curl -N -X POST http://localhost:8007/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Bonjour !"}'
```

## API Endpoints

| MÃ©thode | Endpoint | Description |
|---------|----------|-------------|
| `GET` | `/health` | Health check |
| `GET` | `/test` | Test connexion OpenAI |
| `POST` | `/chat` | Envoie un message, retourne un stream + publie dans RabbitMQ |
| `GET` | `/stream/{session_id}` | SSE - consomme les chunks depuis RabbitMQ |

### POST /chat

**Request:**
```json
{
  "message": "Explique-moi les microservices",
  "session_id": "optional-custom-id"
}
```

**Response:** Stream text/plain + Header `X-Session-ID`

### GET /stream/{session_id}

**Response:** SSE avec events :
```
data: {"chunk": "Bonjour"}

data: {"chunk": " !"}

data: {"type": "done"}
```

## Interface Web

### Lancer l'interface

```bash
# Option 1 : Ouvrir directement le fichier
# Windows
start chat.html

# macOS
open chat.html

# Linux
xdg-open chat.html

# Option 2 : Serveur local (Ã©vite les problÃ¨mes CORS)
python -m http.server 3000
# Puis ouvrir http://localhost:3000/chat.html

# Option 3 : Extension VS Code "Live Server"
# Clic droit sur chat.html â†’ "Open with Live Server"
```

### Configuration

Par dÃ©faut, l'interface se connecte Ã  `http://localhost:8007`. Pour changer l'URL de l'API, modifier la variable dans `chat.html` :

```javascript
const API_URL = 'http://localhost:8007';
```

### Modes disponibles

| Mode | Description | Flux |
|------|-------------|------|
| **RabbitMQ** | DÃ©couplÃ© via message queue | `POST /chat` â†’ RabbitMQ â†’ `SSE /stream/{id}` |
| **Direct** | Stream HTTP classique | `POST /chat` â†’ Stream response |

### FonctionnalitÃ©s

- ğŸ’¬ Chat en temps rÃ©el avec streaming
- ğŸ”„ Switch entre mode RabbitMQ et Direct
- â±ï¸ Timestamps sur les messages
- ğŸ¯ Indicateur de typing pendant la gÃ©nÃ©ration
- ğŸ“Š Status indicators (API, Queue, Stream)
- ğŸ“± Responsive design

## Structure du projet

```
llm_fastapi_mq/
â”œâ”€â”€ main.py                 # Application FastAPI
â”œâ”€â”€ config.py               # Configuration (env vars)
â”œâ”€â”€ requirements.txt        # DÃ©pendances Python
â”œâ”€â”€ Dockerfile              # Image Docker
â”œâ”€â”€ entrypoint.sh           # Script d'entrÃ©e Docker
â”œâ”€â”€ run.sh                  # Script de gestion
â”œâ”€â”€ chat.html               # Interface web
â”œâ”€â”€ .env                    # Variables d'environnement
â”œâ”€â”€ .env.example            # Template env
â””â”€â”€ services/
    â”œâ”€â”€ rabbit_publisher.py # Publisher RabbitMQ
    â””â”€â”€ rabbit_consumer.py  # Consumer RabbitMQ
```

## Scripts

```bash
./run.sh start    # Build + Run
./run.sh stop     # Stop container
./run.sh restart  # Restart
./run.sh logs     # Voir les logs
./run.sh shell    # Shell dans le container
./run.sh test     # Test les endpoints
```

## Scaling

Pour scaler les workers qui consomment les messages :

```bash
# Lancer plusieurs consumers
docker run -d --name worker-1 --env-file .env llm-fastapi-mq
docker run -d --name worker-2 --env-file .env llm-fastapi-mq
```

Avec Kubernetes :
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-stream-worker
spec:
  replicas: 3
  # ...
```

## Troubleshooting

### Connection error

Si vous avez une erreur de connexion OpenAI, vÃ©rifiez que votre `.env` n'a pas de caractÃ¨res `\r` (Windows). Le `config.py` utilise `.strip()` pour nettoyer les variables.

### RabbitMQ timeout

Augmentez le timeout dans `rabbit_consumer.py` si les rÃ©ponses LLM sont longues.

## License

MIT
