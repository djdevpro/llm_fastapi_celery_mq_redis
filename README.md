# ğŸš€ LLM Stream API

> **Scalez votre API LLM** avec Celery + Redis ou RabbitMQ.

[![Python 3.12](https://img.shields.io/badge/python-3.12-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.124-green.svg)](https://fastapi.tiangolo.com)
[![Celery](https://img.shields.io/badge/Celery-5.4-green.svg)](https://docs.celeryq.dev)
[![Docker](https://img.shields.io/badge/Docker-ready-blue.svg)](https://docker.com)

## ğŸ¯ Le problÃ¨me rÃ©solu

Votre API LLM lag quand plusieurs utilisateurs envoient des requÃªtes simultanÃ©ment ? C'est normal : chaque appel OpenAI prend **10-60 secondes** et bloque un worker HTTP.

**Cette architecture rÃ©sout le problÃ¨me** en dÃ©couplant le traitement :
- L'API retourne **immÃ©diatement** (~100ms)
- Les workers traitent les requÃªtes **en parallÃ¨le**
- Le client reÃ§oit la rÃ©ponse via **Server-Sent Events**

---

## ğŸ”„ Deux modes disponibles

| Mode | Backend | Avantages |
|------|---------|-----------|
| **Celery** (recommandÃ©) | Redis | Rate limiting, retry auto, prioritÃ©s, monitoring |
| **RabbitMQ** | RabbitMQ | Legacy, simple |

```bash
# Choisir le mode dans .env
MODE=celery    # ou rabbitmq
```

---

## ğŸ“ Architecture

### Mode Celery + Redis (recommandÃ©) âš¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    POST /chat/async    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚    FastAPI      â”‚
â”‚             â”‚ â—„â”€â”€ task_id, session â”€ â”‚   (main_celery) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                        â”‚
       â”‚                                        â–¼
       â”‚ SSE                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                               â”‚     Redis       â”‚
       â”‚                               â”‚  - Broker       â”‚
       â”‚                               â”‚  - Pub/Sub      â”‚
       â”‚                               â”‚  - Rate limit   â”‚
       â”‚                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                        â”‚
       â”‚                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                               â–¼        â–¼        â–¼
       â”‚                            Celery   Celery   Celery
       â”‚                            Worker   Worker   Worker
       â”‚                               â”‚        â”‚        â”‚
       â”‚ GET /stream/{session_id}      â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                        â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          chunks SSE
```

### Avantages Celery vs RabbitMQ brut

| Feature | RabbitMQ brut | Celery |
|---------|---------------|--------|
| Retry automatique | âŒ Ã€ coder | âœ… `autoretry_for` |
| Backoff exponentiel | âŒ Ã€ coder | âœ… `retry_backoff=True` |
| Rate limiting | âŒ Ã€ coder | âœ… `rate_limit="100/m"` |
| PrioritÃ© des tÃ¢ches | âŒ Ã€ coder | âœ… `queue="high"` |
| Timeout | âŒ Ã€ coder | âœ… `task_time_limit=300` |
| Tracking Ã©tat | âŒ Ã€ coder | âœ… `AsyncResult.status` |
| Monitoring | âŒ Rien | âœ… Flower |

---

## âš™ï¸ Variables d'environnement

```env
# === REQUIS ===
OPENAI_API_KEY=sk-proj-xxxxx

# === MODE (celery ou rabbitmq) ===
MODE=celery

# === REDIS (pour mode celery) ===
REDIS_URL=redis://redis:6379/0

# === RABBITMQ (pour mode rabbitmq, distant CloudAMQP) ===
RABBIT_MQ=amqps://user:pass@coral.rmq.cloudamqp.com/vhost

# === SCALING ===
UVICORN_WORKERS=4       # Workers HTTP
CELERY_CONCURRENCY=4    # Workers Celery (mode celery)
LLM_WORKERS=3           # Workers LLM (mode rabbitmq)
PORT=8007

# === RATE LIMITING ===
LLM_RPM=500             # Requests per minute
LLM_TPM=100000          # Tokens per minute
```

---

## ğŸš€ DÃ©marrage rapide

### 1. Configuration

```bash
cp .env.example .env
# Ã‰diter avec vos clÃ©s
```

### 2. Docker Compose (recommandÃ©)

```bash
# Mode Celery (dÃ©faut)
docker-compose up -d

# VÃ©rifier les logs
docker-compose logs -f llm-api
```

### 3. VÃ©rification

```bash
# Health check
curl http://localhost:8007/health/full

# Mode Celery :
# {"status":"ok","redis":"connected","celery_workers":"active","openai":"configured"}

# Mode RabbitMQ :
# {"status":"ok","rabbitmq":"connected","openai":"configured"}
```

---

## ğŸ“¡ API Endpoints

### Mode Celery (`main_celery.py`)

| MÃ©thode | Endpoint | Description |
|---------|----------|-------------|
| `GET` | `/health` | Health check |
| `GET` | `/health/full` | Statut Redis + Celery + OpenAI |
| `POST` | `/chat` | Mode sync (streaming direct) |
| `POST` | `/chat/async` | **Mode async (Celery task)** âš¡ |
| `GET` | `/chat/{task_id}` | Status d'une tÃ¢che Celery |
| `GET` | `/stream/{session_id}` | Stream SSE depuis Redis |
| `POST` | `/embeddings` | Batch embeddings async |
| `GET` | `/stats` | Stats queues + workers |

### Mode RabbitMQ (`main.py`)

| MÃ©thode | Endpoint | Description |
|---------|----------|-------------|
| `GET` | `/health` | Health check |
| `GET` | `/health/full` | Statut RabbitMQ + OpenAI |
| `POST` | `/chat` | Mode sync (streaming) |
| `POST` | `/chat/async` | Mode async (RabbitMQ) |
| `GET` | `/stream/{session_id}` | Stream SSE depuis RabbitMQ |
| `GET` | `/stats` | TÃ¢ches en attente |

### Exemple : Mode Celery

```bash
# 1. Envoie la requÃªte â†’ retour immÃ©diat
curl -X POST http://localhost:8007/chat/async \
  -H "Content-Type: application/json" \
  -d '{"message": "Explique Docker", "priority": 5}'

# RÃ©ponse :
# {
#   "status": "queued",
#   "task_id": "abc-123",
#   "session_id": "xyz-456",
#   "stream_url": "/stream/xyz-456"
# }

# 2. VÃ©rifier le status de la tÃ¢che
curl http://localhost:8007/chat/abc-123

# 3. Ã‰couter le stream SSE
curl -N http://localhost:8007/stream/xyz-456
# data: {"type":"status","status":"started"}
# data: {"type":"chunk","content":"Docker"}
# data: {"type":"chunk","content":" est"}
# data: {"type":"complete"}
```

---

## ğŸ§ª Tests

```bash
# Tests mode Celery
pytest tests/test_celery.py -v -s

# Tests mode RabbitMQ
pytest tests/test_concurrent.py -v -s

# Tous les tests
pytest tests/ -v -s
```

### PrÃ©requis pour les tests

```bash
# Mode Celery
docker-compose up -d redis
celery -A celery_app worker --loglevel=info -c 4
uvicorn main_celery:app --port 8007

# Mode RabbitMQ
# RabbitMQ distant (CloudAMQP)
uvicorn main:app --port 8007
```

---

## ğŸ“ Structure du projet

```
llm_fastapi_mq/
â”œâ”€â”€ main.py                 # API FastAPI (mode RabbitMQ)
â”œâ”€â”€ main_celery.py          # API FastAPI (mode Celery) âš¡
â”œâ”€â”€ celery_app.py           # Configuration Celery
â”œâ”€â”€ config.py               # Variables d'environnement
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ entrypoint.sh           # Lance API + Workers (auto-dÃ©tecte le mode)
â”œâ”€â”€ chat.html               # Interface web (3 modes)
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ connection_pool.py  # Pool connexions RabbitMQ
â”‚   â”œâ”€â”€ llm_worker.py       # Worker LLM (mode RabbitMQ)
â”‚   â”œâ”€â”€ rabbit_publisher.py
â”‚   â””â”€â”€ rabbit_consumer.py
â”œâ”€â”€ tasks/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ llm_tasks.py        # TÃ¢ches Celery (rate limiting, retry)
â””â”€â”€ tests/
    â”œâ”€â”€ conftest.py
    â”œâ”€â”€ test_celery.py      # Tests mode Celery
    â””â”€â”€ test_concurrent.py  # Tests mode RabbitMQ
```

---

## ğŸ–¥ï¸ Interface Web

```bash
# Ouvrir chat.html dans le navigateur
open chat.html
```

**3 modes disponibles :**
- ğŸŸ¢ **Celery** â€” Streaming via Redis pub/sub
- ğŸ”µ **RabbitMQ** â€” Streaming via RabbitMQ
- âš¡ **Direct** â€” Streaming HTTP direct

---

## ğŸ“Š Scaling

### Mode Celery

```bash
# Plus de workers Celery
CELERY_CONCURRENCY=8

# Ou lancer plusieurs workers
celery -A celery_app worker -c 4 -Q high,default &
celery -A celery_app worker -c 4 -Q low &
```

### Mode RabbitMQ

```bash
# Plus de workers LLM
LLM_WORKERS=10
```

### Configurations recommandÃ©es

| Charge | CELERY_CONCURRENCY | UVICORN_WORKERS |
|--------|-------------------|-----------------|
| Dev | 2 | 1 |
| Petit (10 users) | 4 | 2 |
| Moyen (50 users) | 8 | 4 |
| Production (100+) | 16-32 | 4 |

---

## ğŸ› Troubleshooting

### Redis non connectÃ©

```bash
# VÃ©rifier que Redis tourne
docker-compose ps redis

# VÃ©rifier l'URL Redis
echo $REDIS_URL
# Doit Ãªtre redis://redis:6379/0 dans Docker
```

### Celery workers inactifs

```bash
# VÃ©rifier les workers
celery -A celery_app inspect active

# Voir les queues
celery -A celery_app inspect reserved
```

### Erreur caractÃ¨res Windows

```bash
# Nettoyer les \r du .env
sed -i 's/\r$//' .env
```

### Port dÃ©jÃ  utilisÃ©

```bash
# Trouver le processus
netstat -ano | findstr :8007

# Ou changer le port dans .env
PORT=8008
```

---

## ğŸ”œ Roadmap

- [ ] Quotas par utilisateur
- [ ] Budget tracking (cost per user)
- [ ] Multi-provider fallback (OpenAI â†’ Anthropic â†’ Ollama)
- [ ] Celery Beat pour jobs planifiÃ©s
- [ ] Flower pour monitoring

---

## ğŸ“„ License

MIT

---

<p align="center">
  <b>â­ Si ce projet vous aide, laissez une Ã©toile !</b>
</p>
