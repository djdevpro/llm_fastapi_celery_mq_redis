# LLM Stream + RabbitMQ

POC de streaming LLM avec dÃ©couplage via RabbitMQ. Permet de dÃ©charger le streaming des rÃ©ponses LLM du serveur principal vers des workers indÃ©pendants.

## Architecture

### Mode Synchrone (`/chat`) - CompatibilitÃ©
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     POST /chat      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚    FastAPI      â”‚
â”‚  (Browser)  â”‚ â—„â”€â”€â”€â”€streamâ”€â”€â”€â”€â”€â”€â”€â”€ â”‚    (traite)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Mode Asynchrone (`/chat/async`) - Haute charge âš¡
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    POST /chat/async   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚    FastAPI      â”‚
â”‚  (Browser)  â”‚ â—„â”€â”€{session_id}â”€â”€â”€â”€â”€â”€ â”‚  (fire & forget)â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                       â”‚
       â”‚                                       â”‚ Publie tÃ¢che
       â”‚ SSE                                   â–¼
       â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                              â”‚    RabbitMQ     â”‚
       â”‚                              â”‚   (llm_tasks)   â”‚
       â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                       â”‚
       â”‚                                       â”‚ Consomme
       â”‚                                       â–¼
       â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                              â”‚  LLM Worker(s)  â”‚ x N instances
       â”‚                              â”‚  (llm_worker.py)â”‚
       â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                       â”‚
       â”‚ GET /stream/{session_id}              â”‚ Publie chunks
       â”‚                                       â–¼
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                      â”‚ llm_session_{id}â”‚
                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Avantages du mode asynchrone

| Aspect | Sync (`/chat`) | Async (`/chat/async`) |
|--------|----------------|----------------------|
| Latence HTTP | BloquÃ© pendant gÃ©nÃ©ration | ~50ms retour immÃ©diat |
| Workers HTTP | 1 par requÃªte active | LibÃ©rÃ© instantanÃ©ment |
| ScalabilitÃ© | LimitÃ©e par uvicorn | Workers indÃ©pendants |
| Charge | ~100 req/s | ~1000+ req/s |

## FonctionnalitÃ©s

- ğŸš€ **Streaming LLM** via OpenAI API (gpt-4o-mini)
- ğŸ“¡ **RabbitMQ** pour le dÃ©couplage producteur/consommateur
- ğŸ”„ **SSE** (Server-Sent Events) pour le streaming client
- ğŸ³ **Docker** ready
- âš¡ **Deux modes** : RabbitMQ (async) ou Direct (sync)

## PrÃ©requis

- Docker
- Compte OpenAI (API Key)
- Compte CloudAMQP ou RabbitMQ local

## Installation

### 1. Configuration

```bash
cp .env.example .env
```

Ã‰diter `.env` :

```env
OPENAI_API_KEY=sk-your-openai-api-key
RABBIT_MQ=amqps://user:password@host/vhost
```

### 2. Build & Run

```bash
# Build et lancer
./run.sh start

# Ou manuellement
docker build -t llm-fastapi-mq .
docker run -d --name llm-mq-poc -p 8007:8007 --env-file .env llm-fastapi-mq
```

### 3. Test

```bash
# Health check
curl http://localhost:8007/health

# Test OpenAI
curl http://localhost:8007/test

# Chat avec streaming
curl -N -X POST http://localhost:8007/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Bonjour !"}'
```

## API Endpoints

| MÃ©thode | Endpoint | Description | Mode |
|---------|----------|-------------|------|
| `GET` | `/health` | Health check basique | - |
| `GET` | `/health/full` | Health check + statut RabbitMQ | - |
| `GET` | `/test` | Test connexion OpenAI | - |
| `GET` | `/stats` | TÃ¢ches en attente dans la queue | - |
| `POST` | `/chat` | Streaming synchrone (legacy) | Sync |
| `POST` | `/chat/async` | Fire-and-forget, retourne session_id | **Async** âš¡ |
| `GET` | `/stream/{session_id}` | SSE - consomme les chunks | Async |

### POST /chat

**Request:**
```json
{
  "message": "Explique-moi les microservices",
  "session_id": "optional-custom-id"
}
```

**Response:** Stream text/plain + Header `X-Session-ID`

### GET /stream/{session_id}

**Response:** SSE avec events :
```
data: {"chunk": "Bonjour"}

data: {"chunk": " !"}

data: {"type": "done"}
```

## Interface Web

### Lancer l'interface

```bash
# Option 1 : Ouvrir directement le fichier
# Windows
start chat.html

# macOS
open chat.html

# Linux
xdg-open chat.html

# Option 2 : Serveur local (Ã©vite les problÃ¨mes CORS)
python -m http.server 3000
# Puis ouvrir http://localhost:3000/chat.html

# Option 3 : Extension VS Code "Live Server"
# Clic droit sur chat.html â†’ "Open with Live Server"
```

### Configuration

Par dÃ©faut, l'interface se connecte Ã  `http://localhost:8007`. Pour changer l'URL de l'API, modifier la variable dans `chat.html` :

```javascript
const API_URL = 'http://localhost:8007';
```

### Modes disponibles

| Mode | Description | Flux |
|------|-------------|------|
| **RabbitMQ** | DÃ©couplÃ© via message queue | `POST /chat` â†’ RabbitMQ â†’ `SSE /stream/{id}` |
| **Direct** | Stream HTTP classique | `POST /chat` â†’ Stream response |

### FonctionnalitÃ©s

- ğŸ’¬ Chat en temps rÃ©el avec streaming
- ğŸ”„ Switch entre mode RabbitMQ et Direct
- â±ï¸ Timestamps sur les messages
- ğŸ¯ Indicateur de typing pendant la gÃ©nÃ©ration
- ğŸ“Š Status indicators (API, Queue, Stream)
- ğŸ“± Responsive design

## Structure du projet

```
llm_fastapi_mq/
â”œâ”€â”€ main.py                 # Application FastAPI (routeur)
â”œâ”€â”€ config.py               # Configuration (env vars)
â”œâ”€â”€ requirements.txt        # DÃ©pendances Python
â”œâ”€â”€ Dockerfile              # Image Docker
â”œâ”€â”€ entrypoint.sh           # Script d'entrÃ©e Docker
â”œâ”€â”€ run.sh                  # Script de gestion
â”œâ”€â”€ chat.html               # Interface web
â”œâ”€â”€ .env                    # Variables d'environnement
â”œâ”€â”€ .env.example            # Template env
â””â”€â”€ services/
    â”œâ”€â”€ __init__.py         # Module init
    â”œâ”€â”€ connection_pool.py  # Pool de connexions RabbitMQ (singleton)
    â”œâ”€â”€ llm_worker.py       # Worker LLM indÃ©pendant (scalable)
    â”œâ”€â”€ rabbit_publisher.py # Publisher RabbitMQ (legacy)
    â””â”€â”€ rabbit_consumer.py  # Consumer RabbitMQ
```

## Scripts

```bash
./run.sh start    # Build + Run
./run.sh stop     # Stop container
./run.sh restart  # Restart
./run.sh logs     # Voir les logs
./run.sh shell    # Shell dans le container
./run.sh test     # Test les endpoints
```

## Scaling (Haute charge) âš¡

### Ã‰tape 1 : Lancer le serveur FastAPI

```bash
# Un seul serveur HTTP suffit (il ne fait que router)
./run.sh start
```

### Ã‰tape 2 : Lancer les workers LLM

```bash
# Localement - Plusieurs workers en parallÃ¨le
python -m services.llm_worker &
python -m services.llm_worker &
python -m services.llm_worker &

# Ou avec Docker
docker run -d --name worker-1 --env-file .env llm-fastapi-mq python -m services.llm_worker
docker run -d --name worker-2 --env-file .env llm-fastapi-mq python -m services.llm_worker
docker run -d --name worker-3 --env-file .env llm-fastapi-mq python -m services.llm_worker
```

### Ã‰tape 3 : Utiliser le mode async

```bash
# POST sur /chat/async au lieu de /chat
curl -X POST http://localhost:8007/chat/async \
  -H "Content-Type: application/json" \
  -d '{"message": "Bonjour !"}'

# RÃ©ponse immÃ©diate :
# {"status": "queued", "session_id": "abc-123", "stream_url": "/stream/abc-123"}

# Puis Ã©couter le stream SSE :
curl -N http://localhost:8007/stream/abc-123
```

### Monitoring

```bash
# Voir les tÃ¢ches en attente
curl http://localhost:8007/stats

# Health check complet
curl http://localhost:8007/health/full
```

### Kubernetes (production)

```yaml
# api-deployment.yaml - Serveur HTTP lÃ©ger
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-api
spec:
  replicas: 2  # 2 suffisent (stateless, rapide)
  template:
    spec:
      containers:
      - name: api
        resources:
          limits:
            memory: "256Mi"
            cpu: "200m"
---
# worker-deployment.yaml - Workers LLM (le vrai travail)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-worker
spec:
  replicas: 10  # Scaler selon la charge
  template:
    spec:
      containers:
      - name: worker
        command: ["python", "-m", "services.llm_worker"]
        resources:
          limits:
            memory: "512Mi"
            cpu: "500m"
```

### Calcul du nombre de workers

```
Formule : workers = (requÃªtes/minute) Ã— (temps moyen gÃ©nÃ©ration en minutes)

Exemple :
- 100 requÃªtes/minute
- 30 secondes par gÃ©nÃ©ration (0.5 min)
- Workers nÃ©cessaires = 100 Ã— 0.5 = 50 workers
```

## Troubleshooting

### Connection error

Si vous avez une erreur de connexion OpenAI, vÃ©rifiez que votre `.env` n'a pas de caractÃ¨res `\r` (Windows). Le `config.py` utilise `.strip()` pour nettoyer les variables.

### RabbitMQ timeout

Augmentez le timeout dans `rabbit_consumer.py` si les rÃ©ponses LLM sont longues.

## License

MIT
